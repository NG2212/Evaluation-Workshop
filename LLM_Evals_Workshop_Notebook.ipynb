{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf2ddb4",
   "metadata": {},
   "source": [
    "\n",
    "# LLM Evaluation Workshop — Hands‑On Notebook\n",
    "**Goal:** give you a theory and framework skeleton with examples to evaluate your own agent through **5 phases**:  \n",
    "1) Vibe Checks → 2) Basic Metrics → 3) LLM‑as‑Judge (+RAG) → 4) Component Validation → 5) Production Monitoring (offline harness).\n",
    "\n",
    "Use the **example code** as reference and **replace the `MockAgent` and mock tools** with your own agent and tools. Feel free to use any coding language. \n",
    "\n",
    "## The mock agent in this Notebook is “Account Copilot” Internal Agent that has 3 tools: \n",
    "* calc: aggregator over account metrics (sum/avg/min/max, group-by time/app/source).\n",
    "* policy_kb.search: retrieves policy snippets (RAG)+ citations\n",
    "* actions.recommend: suggests next-best actions based on data patterns and policies.\n",
    "\n",
    "  \n",
    "\n",
    "> This notebook avoids external calls by default. Where relevant, hooks are provided to plug in your own LLM/judge and data sources.\n",
    "> The main goal is to get you to make all the decisions on how to evaluate your agent as a means for \"Eval-driven development\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91a5b3",
   "metadata": {},
   "source": [
    "## 0. Setup — Agent Interface & Mocks\n",
    "**This is just for the example!! Use your agent built in your framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal interfaces. Replace MockAgent with your real agent.\n",
    "# Your task: define agent behavior and tools. \n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "import math, json, random, statistics, re\n",
    "\n",
    "@dataclass\n",
    "class Step:\n",
    "    tool: str\n",
    "    args: Dict[str, Any]\n",
    "    output: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class AgentResult:\n",
    "    final: Dict[str, Any]\n",
    "    trace: List[Step]\n",
    "    termination: str\n",
    "    retrieved_context: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "class MockTools:\n",
    "    # Fake tool implementations for demo purposes only\n",
    "    @staticmethod\n",
    "    def calc(args):\n",
    "        # Pretend to compute DAU sum last 7d\n",
    "        if args.get(\"app\") == \"AcmeApp\" and args.get(\"window\") == \"7d\":\n",
    "            return {\"value\": 124500.0}\n",
    "        return {\"value\": 0.0, \"note\": \"no data in window\"}\n",
    "\n",
    "    @staticmethod\n",
    "    def policy_kb_search(args):\n",
    "        text = \"Raw PII exports are disallowed; use aggregated reports.\"\n",
    "        return {\"hits\": [{\"id\": \"kb#12\", \"text\": text}]}\n",
    "\n",
    "    @staticmethod\n",
    "    def actions_recommend(args):\n",
    "        ctx = [{\"id\": \"policy-ops#34\",\n",
    "                \"text\": \"When uninstall spikes exceed 20% WoW, consider churn-risk audiences and alerting.\"}]\n",
    "        return {\"context\": ctx,\n",
    "                \"recommendation\": \"Create a churn-risk audience and enable an uninstall anomaly alert.\"}\n",
    "\n",
    "class MockAgent:\n",
    "    MAX_STEPS = 4\n",
    "    def run(self, query: str) -> AgentResult:\n",
    "        q = query.lower()\n",
    "        trace = []\n",
    "        # route\n",
    "        if \"dau\" in q or \"cpi\" in q:\n",
    "            tool = \"calc\"; args = {\"metric\": \"DAU\", \"agg\": \"sum\", \"window\": \"7d\", \"app\": \"AcmeApp\"}\n",
    "            out = MockTools.calc(args)\n",
    "            trace.append(Step(tool, args, out))\n",
    "            return AgentResult(final={\"answer\": out[\"value\"], \"confidence\": 0.9}, trace=trace, termination=\"success\")\n",
    "        if \"pii\" in q or \"policy\" in q:\n",
    "            tool = \"policy_kb.search\"; args = {\"query\": query}\n",
    "            out = MockTools.policy_kb_search(args)\n",
    "            trace.append(Step(tool, args, out))\n",
    "            return AgentResult(final={\"answer\": out[\"hits\"][0][\"text\"],\n",
    "                                      \"citation\": out[\"hits\"][0][\"id\"], \"confidence\": 0.85},\n",
    "                               trace=trace, termination=\"success\", retrieved_context=out[\"hits\"])\n",
    "        if \"uninstall\" in q or \"what should i do\" in q or \"recommend\" in q:\n",
    "            tool = \"actions.recommend\"; args = {\"query\": query}\n",
    "            out = MockTools.actions_recommend(args)\n",
    "            trace.append(Step(tool, args, out))\n",
    "            return AgentResult(final={\"recommendation\": out[\"recommendation\"],\n",
    "                                      \"citation\": out[\"context\"][0][\"id\"], \"confidence\": 0.8},\n",
    "                               trace=trace, termination=\"success\", retrieved_context=out[\"context\"])\n",
    "        if \"export raw pii\" in q:\n",
    "            # guardrail refusal example\n",
    "            out = MockTools.policy_kb_search({\"query\": \"PII policy\"})\n",
    "            trace.append(Step(\"policy_kb.search\", {\"query\":\"PII policy\"}, out))\n",
    "            return AgentResult(final={\"answer\": \"I can’t export raw PII. Use aggregated reports.\",\n",
    "                                      \"citation\": out[\"hits\"][0][\"id\"]},\n",
    "                               trace=trace, termination=\"refusal\", retrieved_context=out[\"hits\"])\n",
    "        # default\n",
    "        return AgentResult(final={\"answer\": \"I don't have a tool for that.\", \"confidence\": 0.2},\n",
    "                           trace=trace, termination=\"refusal\")\n",
    "\n",
    "agent = MockAgent()\n",
    "print(\"MockAgent ready. Replace with your own: agent = YourAgent()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870f5aa",
   "metadata": {},
   "source": [
    "## Phase 1 — Vibe Checks (manual smoke tests)\n",
    "\n",
    "**What we do:** Quick manual spot-checks to see if the agent \"feels\" usable and makes sense before investing in formal evaluation.\n",
    "**What are we looking for:**\n",
    "* Does the agent return something at all?\n",
    "* Is the format consistent (short answer, optional notes)?\n",
    "* Does it avoid glaring hallucinations (e.g., making up policies)?\n",
    "* Are recommendations sensible, even if not perfect?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888a1fc",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise:** If you have a working agent, run a few representative questions. Eyeball the answers for sanity: format, usefulness, obvious hallucinations.\n",
    "Else, define what will be main vibe checks once your agent works and move on.\n",
    "\n",
    "> Replace the queries with your own. Keep outputs short and consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d396a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try doing vibe checks that will test using all tools and not just the \"happy path\".\n",
    "queries = [\n",
    "    \"DAU for AcmeApp last 7 days?\",\n",
    "    \"What’s our PII handling policy?\",\n",
    "    \"Uninstalls spiked 30% WoW. What should I do?\"\n",
    "]\n",
    "for q in queries:\n",
    "    res = agent.run(q)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", res.final)\n",
    "    print(\"termination:\", res.termination)\n",
    "    print(\"trace:\", [ (s.tool, s.args) for s in res.trace ])\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84026172",
   "metadata": {},
   "source": [
    "## Phase 2 — Basic Metrics (quick automatic checks)\n",
    "**What we do:** Apply simple, reference-based metrics (like BLEU/ROUGE or string matching) to measure whether outputs roughly align with expectations\n",
    "\n",
    "**How this is done**\n",
    "1. Collect a tiny eval set (5–10 examples) with expected answers\n",
    "2. Apply relevant metrics, e.g.: \n",
    "* IsEqual\n",
    "* ROUGE/ BLEU (text similarity – length)\n",
    "* Contains/ RegexMatch\n",
    "* Schema metch (IsJason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316a3a6",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise:** \n",
    "* Define metrics you should calculate for the entire process, or for components; \n",
    "* Create a tiny eval set with expected outputs and simple metrics:\n",
    "\n",
    "**In this example:**\n",
    "- numeric tolerance for metrics,\n",
    "- contains/exact for policy/action phrases,\n",
    "- optional ROUGE‑1 as a crude overlap score.\n",
    "\n",
    "> Fill/extend `EVAL_SET` with your own cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6540c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable\n",
    "\n",
    "def is_numeric_close(got, exp, tol=0.05):\n",
    "    try:\n",
    "        g = float(str(got).replace(\",\",\"\")); e = float(str(exp).replace(\",\",\"\"))\n",
    "        return abs(g-e) <= tol*max(1.0, abs(e))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def rouge1_recall(pred: str, ref: str) -> float:\n",
    "    # token-level recall\n",
    "    ps = pred.lower().split(); rs = ref.lower().split()\n",
    "    if not rs: return 1.0\n",
    "    from collections import Counter\n",
    "    inter = sum((Counter(ps) & Counter(rs)).values())\n",
    "    return inter / len(rs)\n",
    "\n",
    "EVAL_SET = [\n",
    "    {\"q\":\"DAU for AcmeApp last 7 days?\", \"expected\":\"124500\", \"metric\":\"numeric\"},\n",
    "    {\"q\":\"What’s our PII handling policy?\", \"expected\":\"Raw PII exports are disallowed; use aggregated reports.\", \"metric\":\"rouge\"},\n",
    "    {\"q\":\"Uninstalls spiked 30% WoW. What should I do?\", \"expected\":\"churn-risk audience\", \"metric\":\"contains\"}\n",
    "]\n",
    "\n",
    "scores = []\n",
    "for item in EVAL_SET:\n",
    "    res = agent.run(item[\"q\"])\n",
    "    ans = json.dumps(res.final)  # stringify in case dict\n",
    "    if item[\"metric\"] == \"numeric\":\n",
    "        ok = is_numeric_close(res.final.get(\"answer\"), item[\"expected\"])\n",
    "    elif item[\"metric\"] == \"rouge\":\n",
    "        ok = rouge1_recall(ans, item[\"expected\"]) >= 0.6\n",
    "    elif item[\"metric\"] == \"contains\":\n",
    "        ok = item[\"expected\"].lower() in ans.lower()\n",
    "    else:\n",
    "        ok = False\n",
    "    scores.append((item[\"q\"], ok))\n",
    "    print(f\"{'PASS' if ok else 'FAIL'} — {item['q']}\")\n",
    "print(\"Passed:\", sum(int(ok) for _,ok in scores), \"/\", len(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe9281",
   "metadata": {},
   "source": [
    "## Phase 3 — LLM‑as‑Judge (+ RAG grounding)\n",
    "**What we do:* Score answers on quality and truthfulness with a judging model.\n",
    "For RAG: Include grounding: Is the recommendation attributable to retrieved context?\n",
    "\n",
    "**What we can check (example)**\n",
    "1. Factuality/Correctness\n",
    "2. Grounding (RAG only) – claims supported by retrieved snippets; no outside hallucinations.\n",
    "3. Relevance – stays on task; \n",
    "4. Action/ tool calling quality – recommendation is reasonable and safe for context.\n",
    "5. Format – concise answer; includes citations for policy/RAG outputs.\n",
    "* Aggregate: Weighted average\n",
    "\n",
    "**RAG specific evaluation:**\n",
    "1. Context Precision: % of retrieved tokens actually cited/used by the answer.\n",
    "2. Context Recall: Did we miss obviously relevant chunks for this query?\n",
    "3. Attribution: Every factual sentence in the answer maps to at least one snippet.\n",
    "4. No-Context Leakage: Penalize correct answers that don’t cite the given snippets when they should.\n",
    "\n",
    "**Bias & hygiene:**\n",
    "\n",
    "* Use a different model for judging \n",
    "* Fixed seed / temperature 0\n",
    "* Blind the judge to model identity.\n",
    "* Periodically human-calibrate: sample 10 cases and compare human vs. judge scores; adjust rubric/weights if drift appears\n",
    "\n",
    "**Example prompt for the judge call:**\n",
    "- You are a strict, impartial evaluator for an internal company “Account Copilot” agent.\n",
    "Judge ONLY what is provided. Do not invent facts. If evidence is missing, penalize grounding.\n",
    "\n",
    "Score on a 1–5 scale (1=very poor, 3=acceptable, 5=excellent) using these criteria:\n",
    "\n",
    "1) factuality — are claims internally consistent and (when context is given) accurate?\n",
    "   5: all check out; 3: some uncertainty; 1: clear error or contradiction.\n",
    "2) grounding — are claims attributable to retrieved_context? Penalize unsupported claims.\n",
    "   5: every factual sentence is supported and cites; 3: mixed; 1: mostly unsupported.\n",
    "3) relevance — focused on the user’s question and task type; no filler.\n",
    "4) action_quality — for action/recommendation tasks: safe, concrete, and sensible for AppsFlyer.\n",
    "   If not an action task, set this to null.\n",
    "5) format — concise; includes citation ids for policy/RAG answers; no inner monologue.\n",
    "\n",
    "Weights (default): factuality 0.30, grounding 0.30, relevance 0.15, action_quality 0.20, format 0.05.\n",
    "If any score is null, renormalize remaining weights.\n",
    "\n",
    "Return JSON ONLY with this schema:\n",
    "{\n",
    "  \"scores\": {\"factuality\": int|null, \"grounding\": int|null, \"relevance\": int|null, \"action_quality\": int|null, \"format\": int|null},\n",
    "  \"overall\": float,                     // weighted average, 1–5 rounded to 3 decimals\n",
    "  \"verdict\": \"pass\" | \"borderline\" | \"fail\",\n",
    "  \"issues\": [                           // short machine-parsable flags\n",
    "    \"unsupported_claim\" | \"hallucination\" | \"missing_citation\" | \"unsafe_action\" |\n",
    "    \"off_topic\" | \"format_violation\" | \"policy_mismatch\"\n",
    "  ],\n",
    "  \"citations_used\": [\"doc_id#line\", ...],   // ids the answer actually relied on (if any)\n",
    "  \"rationale\": \"one concise paragraph summarizing why these scores were assigned\"\n",
    "}\n",
    "No preamble, no additional text.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876a414",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise: \n",
    "1. Define all the scoring elements & weights\n",
    "2. Create or mock a gold dataset\n",
    "3. Score outputs on a rubric using a judge model. \n",
    "\n",
    "**The example below includes:**\n",
    "- a **judge hook** (`call_judge_llm`) — replace with your provider call,\n",
    "- a **fallback heuristic judge** so the cell runs offline,\n",
    "- simple **RAG grounding** diagnostics (attribution rate).\n",
    "\n",
    "> Replace rubric/weights and the judge call as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUBRIC_WEIGHTS = {\"factuality\":0.30,\"grounding\":0.30,\"relevance\":0.15,\"action_quality\":0.20,\"format\":0.05}\n",
    "\n",
    "def call_judge_llm(user_query:str, assistant_answer:str, retrieved_context:list)->dict:\n",
    "    \"\"\"Plug your LLM here. Must return:\n",
    "    {\"scores\":{\"factuality\":1-5,\"grounding\":1-5,\"relevance\":1-5,\"action_quality\":1-5,\"format\":1-5},\n",
    "     \"rationale\":\"...\"}\n",
    "    The default below is a simple heuristic so the notebook runs offline.\n",
    "    \"\"\"\n",
    "    # Heuristic: boost if answer mentions key terms & cites context id\n",
    "    ans = assistant_answer.lower()\n",
    "    ctx_text = \" \".join(c.get(\"text\",\"\").lower() for c in retrieved_context)\n",
    "    def s(v): return max(1, min(5, v))\n",
    "    factuality = s(4 if any(x in ans for x in [\"124500\",\"disallowed\",\"audience\",\"alert\"]) else 3)\n",
    "    grounding = s(5 if any(c.get(\"id\",\"\").split(\"#\")[0] in assistant_answer for c in retrieved_context) or (retrieved_context and any(w in ans and w in ctx_text for w in [\"audience\",\"alert\",\"pii\"])) else (3 if retrieved_context else 4))\n",
    "    relevance = s(4 if len(ans) > 0 else 2)\n",
    "    action_quality = s(4 if \"recommendation\" in assistant_answer or \"alert\" in ans or \"audience\" in ans else 3)\n",
    "    fmt = s(4)\n",
    "    return {\"scores\":{\"factuality\":factuality,\"grounding\":grounding,\"relevance\":relevance,\"action_quality\":action_quality,\"format\":fmt},\n",
    "            \"rationale\":\"heuristic offline judge\"}\n",
    "\n",
    "def overall_score(scores:dict)->float:\n",
    "    return round(sum(scores[k]*RUBRIC_WEIGHTS[k] for k in RUBRIC_WEIGHTS),3)\n",
    "\n",
    "def attribution_rate(answer:str, snippets:list)->float:\n",
    "    # naive: count sentences that share terms with any snippet\n",
    "    sents = [s.strip() for s in re.split(r'[.!?]\\s+', answer) if s.strip()]\n",
    "    if not sents: return 1.0\n",
    "    txts = [s.get(\"text\",\"\").lower() for s in snippets]\n",
    "    used = 0\n",
    "    for snt in sents:\n",
    "        snt_l = snt.lower()\n",
    "        if any(any(w in snt_l for w in t.split()[:5]) for t in txts):\n",
    "            used += 1\n",
    "    return used/len(sents)\n",
    "\n",
    "# Run judge on 3 sample cases\n",
    "CASES = [\n",
    "    {\"q\":\"DAU for AcmeApp last 7 days?\"},\n",
    "    {\"q\":\"What’s our PII handling policy?\"},\n",
    "    {\"q\":\"Uninstalls spiked 30% WoW. What should I do?\"}\n",
    "]\n",
    "for c in CASES:\n",
    "    r = agent.run(c[\"q\"])\n",
    "    ans_str = json.dumps(r.final)\n",
    "    j = call_judge_llm(c[\"q\"], ans_str, r.retrieved_context)\n",
    "    ov = overall_score(j[\"scores\"])\n",
    "    ar = attribution_rate(ans_str, r.retrieved_context) if r.retrieved_context else None\n",
    "    print(c[\"q\"], \"-> overall:\", ov, \"| scores:\", j[\"scores\"], \"| attr_rate:\", ar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaeb92e",
   "metadata": {},
   "source": [
    "## Phase 4 — Component Validation (unit + contract tests)\n",
    "\n",
    "- **What do we do:** Test each building block in isolation so failures are obvious and cheap to fix\n",
    "- **What we can test:**\n",
    "1. Correctness & Contracts (schema, results, alignment)\n",
    "2. Robustness & Safety\n",
    "3. Performance & Cost\n",
    "4. Agents: Planning, Tool Use, Guardrails, Convergence \n",
    "5. Multi-agent: Specific agent results, Handoff, Coordination\n",
    "6. RAG: Retrieval & Grounding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bcbdd",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise:** \n",
    "1. Plan how to validate the different components. \n",
    "2. Validate individual components. \n",
    "* In the example below, we show three patterns:\n",
    "- `calc` numeric correctness vs a reference,\n",
    "- `policy_kb.search` retrieval metrics (Recall@K, MRR) + snippet hygiene,\n",
    "- RAG retriever checks for `actions.recommend` (attribution and no‑context behavior).\n",
    "\n",
    "> Replace mock functions with your own; keep the tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- calc correctness ----\n",
    "def reference_sum_dau_last7(app:str)->float:\n",
    "    # Trusted baseline (pretend). Replace with SQL/Pandas.\n",
    "    return 124500.0 if app==\"AcmeApp\" else 0.0\n",
    "\n",
    "def test_calc_sum_last7d():\n",
    "    got = MockTools.calc({\"metric\":\"DAU\",\"agg\":\"sum\",\"window\":\"7d\",\"app\":\"AcmeApp\"})\n",
    "    exp = reference_sum_dau_last7(\"AcmeApp\")\n",
    "    assert abs(got[\"value\"]-exp) <= 0.01*max(1.0,abs(exp))\n",
    "test_calc_sum_last7d()\n",
    "print(\"calc: PASS\")\n",
    "\n",
    "# ---- policy_kb.search retrieval ----\n",
    "GOLD = [\n",
    "  {\"q\":\"What is our PII handling policy?\",\"relevant\":[\"kb#12\"],\"must\":[\"PII\",\"aggregated\"]}\n",
    "]\n",
    "\n",
    "def recall_at_k(ids, relevant, k=5):\n",
    "    return 1.0 if any(r in ids[:k] for r in relevant) else 0.0\n",
    "\n",
    "def mrr_at_k(ids, relevant, k=5):\n",
    "    for i,d in enumerate(ids[:k], start=1):\n",
    "        if d in relevant: return 1.0/i\n",
    "    return 0.0\n",
    "\n",
    "hits = MockTools.policy_kb_search({\"query\":GOLD[0][\"q\"]})[\"hits\"]\n",
    "IDs = [h[\"id\"] for h in hits]\n",
    "assert recall_at_k(IDs, GOLD[0][\"relevant\"], 3) >= 1.0\n",
    "assert mrr_at_k(IDs, GOLD[0][\"relevant\"], 3) >= 1.0\n",
    "text = hits[0][\"text\"]\n",
    "assert all(m.lower() in text.lower() for m in GOLD[0][\"must\"])\n",
    "print(\"policy_kb.search: PASS\")\n",
    "\n",
    "# ---- RAG retriever checks ----\n",
    "def rag_attribution_ok(query:str, phrases:list)->bool:\n",
    "    out = MockTools.actions_recommend({\"query\": query})\n",
    "    txt = \" \".join(s[\"text\"].lower() for s in out[\"context\"])\n",
    "    return all(p in txt for p in [ph.lower() for ph in phrases])\n",
    "\n",
    "assert rag_attribution_ok(\"Uninstalls up 30% WoW\", [\"churn-risk\",\"alerting\"]) == True\n",
    "print(\"RAG retriever: PASS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211ed1f",
   "metadata": {},
   "source": [
    "### Optional — Tool‑Calling Validation on a Gold Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180defc",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise:** Given a table of `question` and expected `tools_to_call`, measure tool plan quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GOLD_TOOLS = [\n",
    "    {\"question\":\"What is 5 * 5?\",\"tools_to_call\":\"calc\"},\n",
    "    {\"question\":\"What’s are the latest models from OpenAI?\",\"tools_to_call\":\"policy_kb.search\"},  # placeholder\n",
    "    {\"question\":\"Uninstalls spiked 30% WoW. What should I do?\",\"tools_to_call\":\"actions.recommend\"}\n",
    "]\n",
    "\n",
    "def parse_tools(s): return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "\n",
    "def prf1(gold_set, used_set):\n",
    "    tp = len(gold_set & used_set); fp = len(used_set - gold_set); fn = len(gold_set - used_set)\n",
    "    P = tp/(tp+fp) if tp+fp else 1.0\n",
    "    R = tp/(tp+fn) if tp+fn else 1.0\n",
    "    F1 = 2*P*R/(P+R) if P+R else 0.0\n",
    "    return P,R,F1\n",
    "\n",
    "def validate_tool_calls(q, expected_csv, ordered=True):\n",
    "    exp = parse_tools(expected_csv)\n",
    "    res = agent.run(q)\n",
    "    used = [s.tool for s in res.trace]\n",
    "    P,R,F1 = prf1(set(exp), set(used))\n",
    "    exact = int(used==exp) if ordered else None\n",
    "    return {\"q\":q,\"used\":used,\"exp\":exp,\"precision\":round(P,3),\"recall\":round(R,3),\"f1\":round(F1,3),\"seq_exact\":exact}\n",
    "\n",
    "for row in GOLD_TOOLS:\n",
    "    print(validate_tool_calls(row[\"question\"], row[\"tools_to_call\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7864b7b",
   "metadata": {},
   "source": [
    "## Phase 5 — Production Monitoring (offline harness)\n",
    "- **What we do:** Test each building block in isolation so failures are obvious and cheap to fix\n",
    "- **Main categories of what we monitor:**\n",
    "1. SLOs & Health\n",
    "2. Tool-Calling Health\n",
    "3. RAG Health\n",
    "4. Safety & Guardrails\n",
    "5. Online Quality: periodic LLM-as-Judge \n",
    "6. Telemetry & Tracing\n",
    "7. Rollouts & Regressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e5892",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise:** \n",
    "1. Plan key metrics and monitors to implement\n",
    "2. Instrument traces and compute a few online‑style KPIs locally. In production you’d emit these as telemetry events (e.g., Prometheus). Here we simulate counters/histograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "metrics = defaultdict(int)\n",
    "latency = []\n",
    "\n",
    "def handle_request(q):\n",
    "    t0 = time.time()\n",
    "    res = agent.run(q)\n",
    "    latency.append((time.time()-t0)*1000.0)\n",
    "    metrics[f\"term_{res.termination}\"] += 1\n",
    "    # tool-level stats\n",
    "    for step in res.trace:\n",
    "        metrics[f\"tool_{step.tool}\"] += 1\n",
    "    # rag stats\n",
    "    if any(s.tool==\"actions.recommend\" for s in res.trace):\n",
    "        has_ctx = any(\"context\" in s.output for s in res.trace)\n",
    "        if not has_ctx: metrics[\"rag_no_context\"] += 1\n",
    "    return res\n",
    "\n",
    "traffic = [\n",
    "    \"DAU for AcmeApp last 7 days?\",\n",
    "    \"What’s our PII handling policy?\",\n",
    "    \"Uninstalls spiked 30% WoW. What should I do?\",\n",
    "    \"Export raw PII for AcmeApp.\"\n",
    "]\n",
    "\n",
    "for q in traffic:\n",
    "    handle_request(q)\n",
    "\n",
    "def pctl(a, p):\n",
    "    if not a: return 0.0\n",
    "    s = sorted(a); k = int((len(s)-1)*p/100)\n",
    "    return s[k]\n",
    "\n",
    "print({\n",
    "    \"success_rate\": metrics[\"term_success\"]/max(1,sum(metrics[k] for k in metrics if k.startswith(\"term_\"))),\n",
    "    \"p95_latency_ms\": round(pctl(latency,95),2),\n",
    "    \"tool_calls\": {k.replace(\"tool_\",\"\"):v for k,v in metrics.items() if k.startswith(\"tool_\")}\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969b82c",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps\n",
    "- Swap `MockAgent` with your real agent; keep the same return contract (`AgentResult`).\n",
    "- Configure evals at all levels\n",
    "- Replace the heuristic judge with your LLM call in `call_judge_llm`.\n",
    "- Wire real references/baselines for `calc`, a real KB for `policy_kb.search`, and your retriever for RAG tests.\n",
    "- Expand gold sets and negative cases; add CI to run Phases 2–4 on every change.\n",
    "- In production, emit per-step telemetry; add canary/shadow gating before full rollout.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
